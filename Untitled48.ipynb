{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5y+i6OAJvWrGTjuBbGtwH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guitsuo/projectobservability/blob/main/Untitled48.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install arize-phoenix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IOsmcPTAvXUf",
        "outputId": "2f28bc1a-bb9a-4ffd-b913-c6f062b4f939"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arize-phoenix\n",
            "  Downloading arize_phoenix-8.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting aioitertools (from arize-phoenix)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting aiosqlite (from arize-phoenix)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arize-phoenix-client (from arize-phoenix)\n",
            "  Downloading arize_phoenix_client-1.0.2-py3-none-any.whl.metadata (817 bytes)\n",
            "Collecting arize-phoenix-evals>=0.13.1 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_evals-0.20.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting arize-phoenix-otel>=0.5.1 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_otel-0.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting authlib (from arize-phoenix)\n",
            "  Downloading Authlib-1.5.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.5.2)\n",
            "Collecting fastapi (from arize-phoenix)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: grpc-interceptor in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.15.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.70.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.28.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (3.1.5)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.26.4)\n",
            "Collecting openinference-instrumentation>=0.1.12 (from arize-phoenix)\n",
            "  Downloading openinference_instrumentation-0.1.22-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.12 (from arize-phoenix)\n",
            "  Downloading openinference_semantic_conventions-0.1.14-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp-1.30.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix)\n",
            "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.37b0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.2.2)\n",
            "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.25.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (18.1.0)\n",
            "Requirement already satisfied: pydantic>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.10.6)\n",
            "Collecting python-multipart (from arize-phoenix)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.38)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix)\n",
            "  Downloading sqlean.py-3.47.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting starlette (from arize-phoenix)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting strawberry-graphql==0.253.1 (from arize-phoenix)\n",
            "  Downloading strawberry_graphql-0.253.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.12.2)\n",
            "Collecting uvicorn (from arize-phoenix)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (14.2)\n",
            "Requirement already satisfied: wrapt>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.17.2)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.253.1->arize-phoenix)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql==0.253.1->arize-phoenix) (2.8.2)\n",
            "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.11/dist-packages (from openinference-instrumentation>=0.1.12->arize-phoenix) (1.16.0)\n",
            "Collecting protobuf<6.0,>=3.20.2 (from arize-phoenix)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (2.27.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.1.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib->arize-phoenix) (43.0.3)\n",
            "Collecting starlette (from arize-phoenix)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette->arize-phoenix) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->arize-phoenix) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->arize-phoenix) (3.0.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.30.0 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.30.0 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp->arize-phoenix) (1.2.18)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp->arize-phoenix) (1.68.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-sdk (from arize-phoenix)\n",
            "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.30.0->opentelemetry-exporter-otlp->arize-phoenix) (2.32.3)\n",
            "Collecting opentelemetry-api (from openinference-instrumentation>=0.1.12->arize-phoenix)\n",
            "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-semantic-conventions (from arize-phoenix)\n",
            "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix)\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (3.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->arize-phoenix) (8.1.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette->arize-phoenix) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.253.1->arize-phoenix) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib->arize-phoenix) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix) (2.22)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.30.0->opentelemetry-exporter-otlp->arize-phoenix) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.30.0->opentelemetry-exporter-otlp->arize-phoenix) (2.3.0)\n",
            "Downloading arize_phoenix-8.6.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.253.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-0.20.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.8.0-py3-none-any.whl (11 kB)\n",
            "Downloading openinference_instrumentation-0.1.22-py3-none-any.whl (19 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.14-py3-none-any.whl (9.2 kB)\n",
            "Downloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlean.py-3.47.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading arize_phoenix_client-1.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading Authlib-1.5.0-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.4/231.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.30.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: sqlean-py, uvicorn, python-multipart, protobuf, openinference-semantic-conventions, Mako, importlib-metadata, graphql-core, aiosqlite, aioitertools, strawberry-graphql, starlette, opentelemetry-proto, opentelemetry-api, alembic, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, fastapi, authlib, arize-phoenix-evals, arize-phoenix-client, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, opentelemetry-exporter-otlp, arize-phoenix-otel, arize-phoenix\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "Successfully installed Mako-1.3.9 aioitertools-0.12.0 aiosqlite-0.21.0 alembic-1.14.1 arize-phoenix-8.6.0 arize-phoenix-client-1.0.2 arize-phoenix-evals-0.20.3 arize-phoenix-otel-0.8.0 authlib-1.5.0 fastapi-0.115.8 graphql-core-3.2.6 importlib-metadata-8.5.0 openinference-instrumentation-0.1.22 openinference-semantic-conventions-0.1.14 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-exporter-otlp-proto-http-1.30.0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 protobuf-5.29.3 python-multipart-0.0.20 sqlean-py-3.47.0 starlette-0.45.3 strawberry-graphql-0.253.1 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "3b9f4cf2a4ab4fd3a435c60e868a5f59"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K0JLCCdd5D0c",
        "outputId": "138fa10f-7f2e-4066-ff10-386725b54c11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import phoenix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "jK2QtHsJ2dxf",
        "outputId": "99677024-3da6-44d0-ddeb-f7725c4ef197"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "multiple exception types must be parenthesized (__init__.py, line 56)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-665e01b90057>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import phoenix\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.11/dist-packages/phoenix/__init__.py\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    except PhoenixError, e:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m multiple exception types must be parenthesized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import phoenix as px\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from phoenix.evals import (\n",
        "    TOOL_CALLING_PROMPT_TEMPLATE,\n",
        "    llm_classify,\n",
        "    GeminiModel\n",
        ")\n",
        "\n",
        "from tqdm import tqdm\n",
        "from phoenix.trace import SpanEvaluations\n",
        "from phoenix.trace.dsl import SpanQuery\n",
        "from openinference.instrumentation import suppress_tracing\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "lQZBKfvjvcMR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import phoenix.evals as pe"
      ],
      "metadata": {
        "id": "P_rpfEeH3Pss"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(pe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5VEjtM44mVJ",
        "outputId": "13bb588c-f58b-4b3f-9008-44fa2c337b98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AnthropicModel',\n",
              " 'BedrockModel',\n",
              " 'CODE_FUNCTIONALITY_PROMPT_RAILS_MAP',\n",
              " 'CODE_FUNCTIONALITY_PROMPT_TEMPLATE',\n",
              " 'CODE_READABILITY_PROMPT_RAILS_MAP',\n",
              " 'CODE_READABILITY_PROMPT_TEMPLATE',\n",
              " 'ClassificationTemplate',\n",
              " 'GeminiModel',\n",
              " 'HALLUCINATION_PROMPT_RAILS_MAP',\n",
              " 'HALLUCINATION_PROMPT_TEMPLATE',\n",
              " 'HALLUCINATION_SPAN_PROMPT_TEMPLATE',\n",
              " 'HUMAN_VS_AI_PROMPT_RAILS_MAP',\n",
              " 'HUMAN_VS_AI_PROMPT_TEMPLATE',\n",
              " 'HallucinationEvaluator',\n",
              " 'LLMEvaluator',\n",
              " 'LiteLLMModel',\n",
              " 'MistralAIModel',\n",
              " 'NOT_PARSABLE',\n",
              " 'OpenAIModel',\n",
              " 'PromptTemplate',\n",
              " 'QAEvaluator',\n",
              " 'QA_PROMPT_RAILS_MAP',\n",
              " 'QA_PROMPT_TEMPLATE',\n",
              " 'QA_SPAN_PROMPT_TEMPLATE',\n",
              " 'RAG_RELEVANCY_PROMPT_RAILS_MAP',\n",
              " 'RAG_RELEVANCY_PROMPT_TEMPLATE',\n",
              " 'REFERENCE_LINK_CORRECTNESS_PROMPT_RAILS_MAP',\n",
              " 'REFERENCE_LINK_CORRECTNESS_PROMPT_TEMPLATE',\n",
              " 'RelevanceEvaluator',\n",
              " 'SQLEvaluator',\n",
              " 'SQL_GEN_EVAL_PROMPT_RAILS_MAP',\n",
              " 'SQL_GEN_EVAL_PROMPT_TEMPLATE',\n",
              " 'SummarizationEvaluator',\n",
              " 'TOOL_CALLING_PROMPT_RAILS_MAP',\n",
              " 'TOOL_CALLING_PROMPT_TEMPLATE',\n",
              " 'TOOL_CALLING_SPAN_PROMPT_TEMPLATE',\n",
              " 'TOXICITY_PROMPT_RAILS_MAP',\n",
              " 'TOXICITY_PROMPT_TEMPLATE',\n",
              " 'ToxicityEvaluator',\n",
              " 'USER_FRUSTRATION_PROMPT_RAILS_MAP',\n",
              " 'USER_FRUSTRATION_PROMPT_TEMPLATE',\n",
              " 'VertexAIModel',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'classify',\n",
              " 'compute_precisions_at_k',\n",
              " 'default_templates',\n",
              " 'download_benchmark_dataset',\n",
              " 'evaluators',\n",
              " 'exceptions',\n",
              " 'executors',\n",
              " 'generate',\n",
              " 'llm_classify',\n",
              " 'llm_generate',\n",
              " 'models',\n",
              " 'retrievals',\n",
              " 'run_evals',\n",
              " 'span_templates',\n",
              " 'templates',\n",
              " 'utils']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "KpIKxnVfvryi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain how AI works\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZmzwtUupP7B",
        "outputId": "9cf4f50b-c6c8-4a0d-8dac-b26621eab493"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down how AI (Artificial Intelligence) works, aiming for clarity and understanding. We'll cover the fundamental concepts without getting overly technical right away.\n",
            "\n",
            "**Core Idea:  Making Machines Think (Sort Of)**\n",
            "\n",
            "At its heart, AI is about creating machines that can perform tasks that typically require human intelligence.  These tasks can include:\n",
            "\n",
            "*   **Learning:**  Improving performance over time based on data.\n",
            "*   **Problem-solving:** Figuring out how to achieve a goal given certain constraints.\n",
            "*   **Reasoning:**  Drawing conclusions from information.\n",
            "*   **Perception:**  Understanding the world through senses (like vision, hearing, etc.).\n",
            "*   **Natural Language Processing (NLP):** Understanding and generating human language.\n",
            "\n",
            "**The Foundation: Algorithms and Data**\n",
            "\n",
            "AI systems are built upon two main pillars:\n",
            "\n",
            "1.  **Algorithms:** These are sets of instructions or rules that a computer follows to solve a specific problem.  Think of them like recipes for intelligence.\n",
            "2.  **Data:** AI algorithms need data to learn from.  The more relevant and high-quality data, the better the AI system will generally perform.\n",
            "\n",
            "**Key Types of AI Approaches**\n",
            "\n",
            "There are several different approaches to achieving AI, but here are some of the most important:\n",
            "\n",
            "*   **Machine Learning (ML):**  This is the most common type of AI in use today. Instead of explicitly programming a machine to do something, we *train* it using data.  The machine learns patterns and relationships in the data and uses those patterns to make predictions or decisions on new, unseen data.\n",
            "\n",
            "    *   **Supervised Learning:**  The algorithm is trained on a labeled dataset. This means that for each input, we also provide the correct output (the \"label\"). The algorithm learns to map inputs to outputs.  *Example:  Training a spam filter. You give it emails labeled as \"spam\" or \"not spam,\" and it learns to identify spam emails.*\n",
            "\n",
            "    *   **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset. It has to find patterns and structures in the data on its own. *Example:  Clustering customers into different groups based on their purchasing behavior.*\n",
            "\n",
            "    *   **Reinforcement Learning:** The algorithm learns by interacting with an environment and receiving rewards or penalties for its actions.  It learns to make decisions that maximize its cumulative reward. *Example:  Training a computer to play a game like chess or Go.*\n",
            "\n",
            "*   **Deep Learning (DL):**  A subfield of machine learning that uses artificial neural networks with many layers (hence \"deep\").  Deep learning is particularly good at handling complex data like images, audio, and text.\n",
            "\n",
            "    *   **Neural Networks:** Inspired by the structure of the human brain, neural networks consist of interconnected nodes (neurons) that process information. The connections between neurons have weights that are adjusted during the learning process.  The more layers, the more complex patterns it can learn.  *Example:  Image recognition, natural language processing, speech recognition.*\n",
            "\n",
            "*   **Rule-Based Systems (Expert Systems):**  These systems use a set of predefined rules to make decisions.  A human expert defines the rules, and the system applies them to new situations.  *Example:  A medical diagnosis system that uses rules to suggest possible diagnoses based on a patient's symptoms.*  This is an older approach and less flexible than machine learning.\n",
            "\n",
            "**How Machine Learning Works: A Simplified Example (Supervised Learning)**\n",
            "\n",
            "Let's say you want to build an AI system to predict the price of a house based on its size.\n",
            "\n",
            "1.  **Collect Data:** You gather a dataset of houses, including their size (in square feet) and their corresponding prices.  This is your \"training data.\"\n",
            "\n",
            "2.  **Choose an Algorithm:** You select a supervised learning algorithm, like linear regression.  Linear regression tries to find a line (in this case, a straight line) that best fits the relationship between size and price.\n",
            "\n",
            "3.  **Train the Model:** You feed the training data into the algorithm. The algorithm adjusts the parameters of the linear regression model (the slope and intercept of the line) to minimize the difference between the predicted prices and the actual prices in the training data.  This is the \"learning\" process.\n",
            "\n",
            "4.  **Test the Model:**  You use a separate set of data (the \"testing data\") to evaluate how well the model generalizes to new, unseen data.  You compare the model's predictions to the actual prices in the testing data.\n",
            "\n",
            "5.  **Make Predictions:** Once you're satisfied with the model's performance, you can use it to predict the price of a new house based on its size.\n",
            "\n",
            "**Deep Learning in a bit more detail**\n",
            "\n",
            "Deep learning uses artificial neural networks with many layers.  Each layer extracts features from the data, and subsequent layers combine those features to learn more complex representations.\n",
            "\n",
            "Imagine trying to teach a computer to recognize cats in images:\n",
            "\n",
            "1.  **Input Layer:** The image is fed into the input layer of the neural network.\n",
            "\n",
            "2.  **Hidden Layers:**\n",
            "    *   The first hidden layer might detect edges and corners in the image.\n",
            "    *   The second hidden layer might combine those edges and corners to detect simple shapes like circles and lines.\n",
            "    *   Subsequent layers might combine these shapes to detect parts of a cat, like ears, eyes, and noses.\n",
            "    *   Finally, a later layer might combine all these features to recognize a cat.\n",
            "\n",
            "3.  **Output Layer:** The output layer outputs the probability that the image contains a cat.\n",
            "\n",
            "**The AI Development Process**\n",
            "\n",
            "Developing an AI system typically involves these steps:\n",
            "\n",
            "1.  **Define the Problem:** What specific problem are you trying to solve with AI?\n",
            "2.  **Gather Data:** Collect and prepare the data that the AI system will learn from.  Data cleaning and preprocessing are crucial.\n",
            "3.  **Choose an Algorithm:** Select the appropriate AI algorithm based on the problem and the data.\n",
            "4.  **Train the Model:** Train the AI model using the training data.\n",
            "5.  **Evaluate the Model:** Evaluate the model's performance using testing data.\n",
            "6.  **Tune the Model:** Adjust the model's parameters and hyperparameters to improve its performance.  This is an iterative process.\n",
            "7.  **Deploy the Model:** Deploy the trained model into a real-world application.\n",
            "8.  **Monitor and Maintain:** Continuously monitor the model's performance and retrain it as needed to maintain its accuracy and effectiveness.\n",
            "\n",
            "**Challenges and Limitations**\n",
            "\n",
            "*   **Data Requirements:** AI, especially deep learning, often requires massive amounts of data.\n",
            "*   **Bias:** AI systems can perpetuate and amplify biases present in the training data.  This can lead to unfair or discriminatory outcomes.\n",
            "*   **Explainability:**  Deep learning models can be difficult to understand. It's often hard to know why a model made a particular decision. This is known as the \"black box\" problem.\n",
            "*   **Overfitting:**  A model can become too specialized to the training data and perform poorly on new data.\n",
            "*   **Ethical Considerations:**  AI raises many ethical concerns, such as job displacement, privacy, and the potential for misuse.\n",
            "\n",
            "**In Summary**\n",
            "\n",
            "AI is a broad field encompassing various techniques aimed at creating machines that can perform tasks requiring human intelligence.  Machine learning, especially deep learning, is a dominant approach that relies on training algorithms with data to learn patterns and make predictions. While AI offers tremendous potential, it's important to be aware of its limitations and ethical considerations.\n",
            "\n",
            "I hope this explanation helps!  Let me know if you have any more questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOp0PqR8rkM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}